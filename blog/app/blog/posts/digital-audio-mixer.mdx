---
title: 'Digital Audio Mixer: A from-scratch digital mixer featuring AI-powered automix (WIP)'
publishedAt: '2024-07-25'
summary: 'Some of my thoughts, research, and experimentation on constructing a digital audio mixer'
image: '/blog-assets/digital-audio-mixer/images/Band.jpg'
---


# The Problem and the Idea

![The band](/blog-assets/digital-audio-mixer/images/Band.jpg)

<center>**Source: [Lovebirds & The Fools](https://www.instagram.com/p/C3ThdSFrfM8/?utm_source=ig_web_copy_link&igsh=MzRlODBiNWFlZA==)**</center>

As someone who's been involved in a band, I've been made acutely aware of the bulk and setup overhead involved with running a live gig. 
This includes but is not limited to: 
* Lugging around two cars worth of amps, pedalboards, synths, cables, etc.
* Plugging in a rats nest of cables
* Repeatedly yelling at guitarists to turn their amps up and down. 
    * The real life version of binary search (but for gain staging)

Getting everyone plugged in and set up + bashing out a competent mix requires some inhuman amount of patience and speed. As a musician and engineer, I felt that there had to be a way to optimize this workflow. 
The somewhat straightforward solution is to plug into a single mixer board and run everything through a PA. This improves on some aspects of setup. A consolidated control center and patch bay can improve the speed
of achieving a competent mix. However, many challenges associated with gain staging, effects, and parameter recall aren't really solved until you start getting into the realm of expensive digital mixers. 
So, I had an idea. I am working alongside some of my fellow band members to bring to life an economical, digital mixer with an AI powered auto-mix algorithm to streamline setup. What does this look like in design? 
Keep reading, and I'll explain the details. 

# Form Factor

![Mixer Mockup](/blog-assets/digital-audio-mixer/images/MixerMockupCropped.png)

This is an early mockup of what the device might look like. It's optimistically slim and features touchscreen control, a plethora of inputs, and knobs for gain/volume. 
It follows a conventional form factor for digital mixers. However, future iterations may eliminate physical control entirely. 

The vision we had was to control the mixer through an app connected over LAN. This is a standard industry practice, however it's only found in high-end, static setups 
located in concert venues. It's our vision to bring this to something with a smaller, mobile form factor. This will enable the band's engineer to make mixes on their phone or tablet from audience perspective,
which is crucial to a pleasing and speedy mix. Without wireless control, the engineer would have to repeatedly make trips between the audience's position and the mixer board.

The board I/O may be iterated upon in the future. We've seen mobile mixers on the market with convenient AUX ins and Bluetooth I/O. This allows the band to easily incorporate backing tracks into the performance.

# Mixer Architecture and Part Selection

![Mixer Block Diagram](/blog-assets/digital-audio-mixer/images/MixerArchitecture.png)

Here's a block diagram of the mixer. Most of it falls under the category of 'reinventing the wheel.' Communication between components is handled by I2C and I2S for control signals and audio streams, respectively. 

## Microcontrollers

The most unconventional design feature is the use of a Raspberry Pi SBC in combination with an xCORE microcontroller. We originally intended to use just a Raspberry Pi for all DSP, control, and AI purposes. However, we quickly
ran into hurdles with its IO and the multi-threaded nature of the project. With ~8 streams of audio, the 4 cores of the Pi would run us a huge challenge of parallel processing. No semaphores and locks for us today, thanks. 

With that in mind, we took a look at what the industry uses and came across the XMOS xCORE line of microcontrollers. The selling point of the xCORE MCUs is its extremely multi-threaded nature, 
with MCUs available in up to 32 core configurations. This would be ample for our DSP purposes.
We decided to keep the Pi around, partly because we had it, and partly because it would make our life easy in terms of smashing together a usable GUI and running our AI model. If this product ever
makes it to production, we would probably port over the AI capabilities to the xCORE (with their new line of chips featuring a dedicated vector ALU!) and control to a STM32 chip or something similar. 

## Electrical Hardware

The hardware of the mixer needs to handle two main tasks, amplication and conversion. I'm currently working on a prototype microphone preamp, and I will write up on it soon. Preamp design is nifty! I'm basing my design off the Analog Devices SSM2019 
amplification IC. 

# Auto-Mix Feature

One of the novel features we envisioned was an AI-powered automix feature. The workflow would go something like this: 

The band engages and configures the auto-mix function, selecting genre and whether or not creative effects should be applied. They then plug into the board and do soundcheck, playing through a section of their song. After that, the 
board spits out a mix configuration, setting levels, doing basic EQ, and applying genre appropriate effects, if desired. 

Below are the main components of the workflow/software. We'll get into the software below.

![Mixer Workflow](/blog-assets/digital-audio-mixer/images/MixerWorkflow.png)

The essence of the mixing software can be broken up into two stages: classification and algorithm. Let's walk through them.

## Classification

In order for the algorithm to work, we need to figure out what types of instruments and sounds are connected to all the inputs, just like a real mix engineer. Is it a screaming lead guitar? Is it a smooth pad? Is it a lead vocal? Knowing such
classifications is essential to a mix done by a real engineer and will be crucial to the algorithmic portion of our mixing feature as well. 

### Classification: Overview and Challenges

One of the main challenges is figuring out what categories to classify sounds into. To achieve the granularity that a real mix engineer could pull off would require an unthinkably large volume of training data. Also, as admittedly amateur engineers, we may
not even be able to envision all of the niche, wacky sounds that people might feed through our mixer. To get past this problem, I had to put on my thinking cap and draw some generalizations about how mixing works. Let's take a crack at it. 

Fundamentally, mixing is about fitting sounds in a finite space composed of frequency and position/panning (if in stereo). This is in part due to a psychoacoustic effect known as frequency masking. If two sounds of similar frequency are played at the same position in 
space, they will "mask," producing a muddy, unpleasant result where neither sound is heard particularly well. In order to unmask two sounds, you can mold the frequency spectrum of one or both sounds so that they are moved out of the way of each other in the 
frequency domain or you can physically move them in space so they are no longer on top of each other. 

Using this knowledge, it makes sense to classify sounds based on their frequency rather than their specific identity as an instrument or noise. So with that, we can build a set of training data with many fewer categories: low frequency/bass, mid-range, and
high-range. These categories will make up one dimension of the classication. 

There is another facet of mixing that informs the classification categories. This facet is not so rigorously defined. I like to simply call it "distance." If you envision a song as a performance on a stage, you have several "layers" of performers at different distances from the audience.
You have the lead layer, which includes the lead vocalist and any solo instrumentation. They stand up front on the stage. You have the backing layer, which includes most of the instrumentation, particularly the rhythmic elements.
You can envision them playing behind the lead vocalist and soloists. You also occasionally have a distant layer, which may consist of faint background vocals, buried ad libs, and ambient, atmospheric noises. 
In each mix, you have to place elements in each layer depending on their role in the arrangement. The classification model must do this as well. So we have one more dimension of classification consisting of three categories, lead, backing, and distant. 

There's an old YouTube video on mixing that summarizes these concepts well with its thumbnail. Here it is for a bit of a visual representation. 

![Art of Mixing Diagram](/blog-assets/digital-audio-mixer/images/artOfMixingDiagram.jpg)

<center>**Source: [The Art of Mixing](https://www.youtube.com/watch?v=TEjOdqZFvhY)**</center>

So, if you do the math, that's 3 * 3 = 9 classes which we have to collect data for. That's so doable! However, we may still have to introduce some extra granularity, especially if we plan on introducing the ability to automatically configure creative effects. 
For several common instruments (e.g. electric guitar, bass, etc.), we may actually train the model to recognize the specific instrument. That way, we can apply appropriate effects based on the user-selected genre. If the performers want to play
alt-rock, we can apply a healthy dose of distortion to the lead guitars. If they want to play something psychedelic, we can put a hazy flanger and huge reverb on the midrange elements. The next section will go into more detail, but this is the basic
justification behind finer granularity in the classification of instruments. 

### Classification: AI Details

So, how are we actually going to acquire data and set up training and execution of our classification model? Let's begin with data gathering. 

For prototyping, I stumbled upon a gold mine of data: Logic Pro's included loops and sample packs. For the uninitiated, Logic Pro is a popular DAW (Digital Audio Workstation) from Apple. It comes bundled with thousands of loops and samples, 
intended for music producers to make their next big hit. I am going to repurpose them for our model training (Note: We would probably get sued to oblivion by Apple if we were to put this mixer into production with Logic Pro training data. Again, this is
just for the prototype). As a proof of concept, I was able to cobble together over 700 different audio files across 5 categories and use them to train a miniature classification model.

Speaking of the model, how are we going to architect it? It's relatively simple. First, we must convert our audio training data into a visual representation. We do that by creating spectrograms of our audio data,
which plot frequency over time. In order to optimize this presentation, we will also use the Mel scale instead of the normal decibel scale. The Mel scale differs from the decibel scale in that its values scale linearly according to human perception of volume. 
So, something that is twice as loud to us will be twice the value on the Mel-scale. This is in contrast to the logarithmic decibel scale, where a two times increase in volume is a 3 decibel jump. Not very intuitive. Scaling our spectrograms in this linear fashion
will give us a better visual representation of audio, leading to better results through visual AI analysis. 

Given that this is now a visual classification problem, anyone familiar with AI will now guess that the obvious solution is a convolutional neural network. I'm not really enough of an AI nerd (yet) to fully explain a CNN, but just know that it 
is (was? up until transformers came along) the de-facto method for classifying images. The aforementioned proof of concept is a CNN trained with Python and the FastAI library. The FastAI library is essentially a slightly abstracted and simplified
version of PyTorch. I loaded my 700 pieces of data and trained a model. For CNNs, the FastAI library defaults to performing transfer learning on the pre-existing RESNET (18 or 34, I'm not sure) model. It delivers fantastic results, with a 
validation accuracy of 90 percent! Check out the model at the link below.

<center>**[Link: AI Instrument Classifier](https://joshieyu-audioclassifier.hf.space/)**</center>

Is this what our final model is going to look like? Not exactly. For one, we need something that runs well on embedded systems. PyTorch isn't going to cut it. In my research, I discovered Google's YAMNET audio classification model. 
It operates off the same principles as my demo model, just with much more training data and a more robust architecture (with about half a million parameters). It delivers over 500 classes of granularity and native support for Tensorflow Lite, 
an embedded AI platform supported by both our Raspberry Pi and XCORE MCU. It was almost perfect for our needs! Given that we don't need all 500+ categories, we will have to perform some transfer learning to narrow down the categories. Also, we wish to introduce 
the aforementioned classification dimensions of frequency and distance. 

## Algorithm

Now that we have sounds classified by their frequency content and arrangement distance, we can work with the algorithm to position them in our mix. In its most basic 
form, the algorithm simply normalizes the audio streams to an appropriate level and pans it appropriately if operating in stereo. If it's a lead vocal, that will
be classified as a midrange element placed up front, panned center. It'll be louder than the rest of the 

# That's it.

That's all I have for now. I'll be updating this blog with updates as the project progresses. Stay tuned and feel free to message me about this project or anything related to music/tech! 







